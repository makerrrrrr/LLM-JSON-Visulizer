# ppt_layout_multi_model.py
import argparse
import asyncio
import json
import os
import time
from datetime import datetime

import yaml
from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()


def load_prompt(file_path="prompt.yaml"):
    with open(file_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


class ModelConfigManager:
    """æ¨¡å‹é…ç½®ç®¡ç†å™¨ - å€Ÿé‰´Paper2Posterçš„è®¾è®¡æ€è·¯"""

    def __init__(self):
        self.model_configs = {
            # OpenAI ç³»åˆ—
            'gpt-4o': {
                'provider': 'openai',
                'model_name': 'gpt-4o',
                'api_key_env': 'OPENAI_API_KEY',
                'base_url_env': 'OPENAI_BASE_URL',
                'base_url_default': 'https://api.openai.com/v1'
            },
            'gpt-4o-mini': {
                'provider': 'openai',
                'model_name': 'gpt-4o-mini',
                'api_key_env': 'OPENAI_API_KEY',
                'base_url_env': 'OPENAI_BASE_URL',
                'base_url_default': 'https://api.openai.com/v1'
            },
            'gpt-4': {
                'provider': 'openai',
                'model_name': 'gpt-4',
                'api_key_env': 'OPENAI_API_KEY',
                'base_url_env': 'OPENAI_BASE_URL',
                'base_url_default': 'https://api.openai.com/v1'
            },

            # é€šä¹‰åƒé—®ç³»åˆ—
            'qwen-plus': {
                'provider': 'qwen',
                'model_name': 'qwen-plus',
                'api_key_env': 'QWEN_API_KEY',
                'base_url_env': 'QWEN_BASE_URL',
                'base_url_default': 'https://dashscope.aliyuncs.com/compatible-mode/v1'
            },
            'qwen-max': {
                'provider': 'qwen',
                'model_name': 'qwen-max',
                'api_key_env': 'QWEN_API_KEY',
                'base_url_env': 'QWEN_BASE_URL',
                'base_url_default': 'https://dashscope.aliyuncs.com/compatible-mode/v1'
            },
            'qwen-turbo': {
                'provider': 'qwen',
                'model_name': 'qwen-turbo',
                'api_key_env': 'QWEN_API_KEY',
                'base_url_env': 'QWEN_BASE_URL',
                'base_url_default': 'https://dashscope.aliyuncs.com/compatible-mode/v1'
            },

            # DeepSeek ç³»åˆ—
            'deepseek-reasoner': {
                'provider': 'deepseek',
                'model_name': 'deepseek-reasoner',
                'api_key_env': 'DEEPSEEK_API_KEY',
                'base_url_env': 'DEEPSEEK_BASE_URL',
                'base_url_default': 'https://api.deepseek.com/v1'
            },
            'deepseek-chat': {
                'provider': 'deepseek',
                'model_name': 'deepseek-chat',
                'api_key_env': 'DEEPSEEK_API_KEY',
                'base_url_env': 'DEEPSEEK_BASE_URL',
                'base_url_default': 'https://api.deepseek.com/v1'
            },

            # æœ¬åœ°æ¨¡å‹
            'llama-3.1-8b': {
                'provider': 'local',
                'model_name': 'llama-3.1-8b',
                'api_key_env': 'LOCAL_API_KEY',
                'base_url_env': 'LOCAL_BASE_URL',
                'base_url_default': 'http://localhost:8000/v1'
            }
        }

    def get_config(self, model_name):
        """è·å–æ¨¡å‹é…ç½®"""
        if model_name not in self.model_configs:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")
        return self.model_configs[model_name]

    def get_available_models(self):
        """è·å–æ‰€æœ‰å¯ç”¨æ¨¡å‹"""
        return list(self.model_configs.keys())

    def create_client(self, model_name):
        """æ ¹æ®æ¨¡å‹é…ç½®åˆ›å»ºå®¢æˆ·ç«¯"""
        config = self.get_config(model_name)

        api_key = os.getenv(config['api_key_env'])
        base_url = os.getenv(config['base_url_env'],
                             config['base_url_default'])

        if not api_key:
            raise ValueError(f"æœªè®¾ç½®ç¯å¢ƒå˜é‡: {config['api_key_env']}")

        return OpenAI(
            api_key=api_key,
            base_url=base_url
        )


class MultiModelGenerator:
    """å¤šæ¨¡å‹PPTå¸ƒå±€ç”Ÿæˆå™¨"""

    def __init__(self):
        self.config_manager = ModelConfigManager()

    async def generate_layout_json(self, content: str, model_name: str) -> str:
        """ä½¿ç”¨æŒ‡å®šæ¨¡å‹ç”ŸæˆPPTå¸ƒå±€JSON"""

        # è·å–æ¨¡å‹é…ç½®å¹¶åˆ›å»ºå®¢æˆ·ç«¯
        client = self.config_manager.create_client(model_name)
        config = self.config_manager.get_config(model_name)

        system_prompt = load_prompt()["system prompt"]

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"è¯·ä¸ºä»¥ä¸‹å†…å®¹ç”ŸæˆPPTå¸ƒå±€JSONï¼š\n\n{content}"}
        ]

        print(f"ï¿½ï¿½ æ­£åœ¨ä½¿ç”¨ {model_name} ç”ŸæˆJSON...")
        start_time = time.time()

        try:
            response = client.chat.completions.create(
                model=config['model_name'],
                messages=messages,
                temperature=0.1
            )

            end_time = time.time()
            elapsed_time = end_time - start_time
            print(f"âœ… {model_name} APIè°ƒç”¨å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")

            return response.choices[0].message.content

        except Exception as e:
            print(f"âŒ {model_name} APIè°ƒç”¨å¤±è´¥: {e}")
            raise


async def main():
    parser = argparse.ArgumentParser(description='å¤šæ¨¡å‹PPTå¸ƒå±€ç”Ÿæˆå™¨')
    parser.add_argument('--model', type=str, default='qwen-max',
                        help='è¦ä½¿ç”¨çš„æ¨¡å‹åç§°')
    parser.add_argument('--list-models', action='store_true',
                        help='åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹')
    parser.add_argument('--prompt-file', type=str, default='prompt.yaml',
                        help='prompté…ç½®æ–‡ä»¶è·¯å¾„')

    args = parser.parse_args()

    generator = MultiModelGenerator()

    # å¦‚æœç”¨æˆ·è¦æ±‚åˆ—å‡ºæ¨¡å‹
    if args.list_models:
        models = generator.config_manager.get_available_models()
        print("ğŸ“‹ å¯ç”¨æ¨¡å‹åˆ—è¡¨:")
        for i, model in enumerate(models, 1):
            config = generator.config_manager.get_config(model)
            print(f"  {i}. {model} ({config['provider']})")
        return

    model_name = args.model

    # éªŒè¯æ¨¡å‹æ˜¯å¦æ”¯æŒ
    try:
        generator.config_manager.get_config(model_name)
    except ValueError as e:
        print(f"âŒ {e}")
        print("ä½¿ç”¨ --list-models æŸ¥çœ‹æ‰€æœ‰å¯ç”¨æ¨¡å‹")
        return

    # éªŒè¯promptæ–‡ä»¶
    if not os.path.exists(args.prompt_file):
        print(f"âŒ æ‰¾ä¸åˆ°promptæ–‡ä»¶: {args.prompt_file}")
        return

    # è®°å½•æ€»å¼€å§‹æ—¶é—´
    total_start_time = time.time()

    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
    timestamp = datetime.now().strftime("%m%d%H")
    output_folder = f"{timestamp}_{model_name}_output"

    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
        print(f"ğŸ“ åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹: {output_folder}")

    # åŠ è½½æ‰€æœ‰prompt
    try:
        prompts = load_prompt(args.prompt_file)
    except Exception as e:
        print(f"âŒ åŠ è½½promptæ–‡ä»¶å¤±è´¥: {e}")
        return

    # éå†æ‰€æœ‰user prompt
    for i in range(1, 9):
        prompt_key = f"user prompt{i}"

        if prompt_key in prompts:
            content = prompts[prompt_key]
            print(f"\nğŸ”„ æ­£åœ¨å¤„ç† {prompt_key}...")

            start_time = time.time()
            try:
                json_result = await generator.generate_layout_json(content, model_name)

                # å»é™¤ markdown ä»£ç å—åŒ…è£¹
                if json_result.strip().startswith("```"):
                    json_result = json_result.strip()
                    if json_result.startswith("```json"):
                        json_result = json_result[len("```json"):].strip()
                    elif json_result.startswith("```"):
                        json_result = json_result[len("```"):].strip()
                    if json_result.endswith("```"):
                        json_result = json_result[:-3].strip()

                # å°è¯•è§£æJSONä»¥ç¡®ä¿æ ¼å¼æ­£ç¡®
                parsed_json = json.loads(json_result)

                # è®¡ç®—è€—æ—¶
                end_time = time.time()
                elapsed_time = end_time - start_time

                # ç”Ÿæˆæ–‡ä»¶å
                filename = f"ppt_layout_{i}_{elapsed_time:.1f}s.json"
                filepath = os.path.join(output_folder, filename)

                # ä¿å­˜åˆ°æ–‡ä»¶
                with open(filepath, "w", encoding="utf-8") as f:
                    json.dump(parsed_json, f, ensure_ascii=False, indent=2)

                print(
                    f"âœ… {prompt_key} å·²ä¿å­˜åˆ°: {filepath} (è€—æ—¶: {elapsed_time:.2f}ç§’)")

            except json.JSONDecodeError as e:
                end_time = time.time()
                elapsed_time = end_time - start_time

                print(f"âŒ {prompt_key} JSONæ ¼å¼é”™è¯¯: {e} (è€—æ—¶: {elapsed_time:.2f}ç§’)")
                filename = f"ppt_layout_{i}_{elapsed_time:.1f}s_raw.txt"
                filepath = os.path.join(output_folder, filename)
                with open(filepath, "w", encoding="utf-8") as f:
                    f.write(json_result)
                print(f"åŸå§‹å†…å®¹å·²ä¿å­˜åˆ°: {filepath}")

            except Exception as e:
                end_time = time.time()
                elapsed_time = end_time - start_time
                print(f"âŒ {prompt_key} å¤„ç†å¤±è´¥: {e} (è€—æ—¶: {elapsed_time:.2f}ç§’)")
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ° {prompt_key}")

    # è®¡ç®—æ€»è€—æ—¶
    total_end_time = time.time()
    total_elapsed_time = total_end_time - total_start_time

    print(f"\nğŸ‰ æ‰€æœ‰å¤„ç†å®Œæˆï¼æ–‡ä»¶ä¿å­˜åœ¨: {output_folder}/")
    print(f"â±ï¸ æ€»è€—æ—¶: {total_elapsed_time:.2f}ç§’")

if __name__ == "__main__":
    asyncio.run(main())
